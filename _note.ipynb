{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d092d0-690b-4499-89ce-3f6a6a91f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4a23f-3ae7-4079-9092-efc4526f7fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import get_model\n",
    "from utils.tools import AttrDict\n",
    "import easydict\n",
    "import torch\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "args = easydict.EasyDict()\n",
    "args['restore_step'] = 56000\n",
    "args['preprocess_config'] = './config/kss/preprocess.yaml'\n",
    "args['synthesizer_config'] = './config/kss/model.yaml'\n",
    "args['generator_config'] = './config/kss/config_v1.json'\n",
    "args['train_config'] = './config/kss/train.yaml'\n",
    "\n",
    "preprocess_config = yaml.load(open(args.preprocess_config, \"r\"), Loader=yaml.FullLoader)\n",
    "synthesizer_config = yaml.load(open(args.synthesizer_config, \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(args.train_config, \"r\"), Loader=yaml.FullLoader)\n",
    "with open(args.generator_config, \"r\", encoding='utf8') as j:\n",
    "    generator_config = json.loads(j.read())\n",
    "    generator_config = AttrDict(generator_config)\n",
    "\n",
    "configs = (preprocess_config, [synthesizer_config, generator_config], train_config)\n",
    "\n",
    "model = get_model(args, configs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d6e0e-16cc-4e00-be04-6b3362322c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tools import to_device_inference, intersperse\n",
    "from text import text_to_sequence\n",
    "import json\n",
    "\n",
    "\n",
    "def get_speaker_ids(path):\n",
    "    with open(path, 'r', encoding='utf8') as j:\n",
    "        speaker_ids = json.loads(j.read())\n",
    "    return speaker_ids\n",
    "\n",
    "def get_speaker_id(speaker, speaker_ids):\n",
    "    return speaker_ids[speaker]\n",
    "\n",
    "def get_text(sentence, preprocess_config):\n",
    "    sequence = text_to_sequence(sentence)\n",
    "    if preprocess_config[\"preprocessing\"][\"text\"][\"use_intersperse\"]:\n",
    "        sequence = intersperse(sequence, 0)\n",
    "    return sequence\n",
    "\n",
    "def get_pairs(speaker:str, sentence:str, speaker_ids:dict, preprocess_config:dict):\n",
    "    speaker_id = torch.LongTensor([get_speaker_id('kss', speaker_ids, )]).long()\n",
    "    sequence = torch.LongTensor(get_text(sentence, preprocess_config)).unsqueeze(0).long()\n",
    "    src_lens = torch.LongTensor([sequence.size(0)]).long()\n",
    "    max_src_len = src_lens.max().long()\n",
    "    return (speaker_id, sequence, src_lens, max_src_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbde24-4841-4beb-a252-50e0e8883f73",
   "metadata": {},
   "source": [
    "## Normal Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8393896-e57b-4187-9d92-9dd980ea4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_ids = get_speaker_ids('./preprocessed_data/kss/speakers.json')\n",
    "speaker = 'kss'\n",
    "sentence = '안녕하세요? 만나서 반갑습니다'\n",
    "\n",
    "pairs = to_device_inference(\n",
    "    get_pairs(speaker, sentence, speaker_ids, preprocess_config), \n",
    "    device\n",
    ")\n",
    "\n",
    "controls = {\n",
    "    'p_control': 1.0, \n",
    "    'e_control': 1.0, \n",
    "    'd_control': 1.0, \n",
    "    'noise_scale': 0.8, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f0b359-87bb-454e-a13e-9a03e6a6c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import AudioTextProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline\n",
    "processor = AudioTextProcessor(preprocess_config)\n",
    "hop_size = processor.preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "\n",
    "output_gen = model(*(pairs), **(controls), gen=True)\n",
    "wav_gen, wav_gen_len = output_gen[0], output_gen[9][0].item() * hop_size\n",
    "wav_gen = wav_gen[0,0,:wav_gen_len]\n",
    "mel_gen = processor.get_mel_energy(wav_gen.detach().cpu().numpy())[0]\n",
    "\n",
    "plt.imshow(mel_gen, origin='lower', aspect='auto')\n",
    "plt.show()\n",
    "\n",
    "sr = preprocess_config['preprocessing']['audio']['sampling_rate']\n",
    "display(ipd.Audio(wav_gen.detach().cpu().numpy(), rate=sr, normalize=False, autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1901a2a-7288-46df-8ede-04a777c30d26",
   "metadata": {},
   "source": [
    "## Voice Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c480e89a-97bf-45b1-9b18-60b988f31609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# voice conversion\n",
    "speaker_ids = get_speaker_ids('./preprocessed_data/kss/speakers.json')\n",
    "speaker = 'kss'\n",
    "sentence = '안녕하세요? 만나서 반갑습니다'\n",
    "\n",
    "pairs = to_device_inference(\n",
    "    get_pairs(speaker, sentence, speaker_ids, preprocess_config), \n",
    "    device\n",
    ")\n",
    "\n",
    "speaker_id, sequence, src_lens, max_src_len = pairs\n",
    "target_speaker_id = speaker_id # set another speaker id if you have multi-speaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ca0a9-a022-4294-93a6-415656f37254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import AudioTextProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "%matplotlib inline\n",
    "processor = AudioTextProcessor(preprocess_config)\n",
    "hop_size = processor.preprocess_config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n",
    "sr = preprocess_config['preprocessing']['audio']['sampling_rate']\n",
    "\n",
    "# normal synthesis\n",
    "output_gen = model(*(pairs), **(controls), gen=True)\n",
    "wav_gen, wav_gen_len = output_gen[0], output_gen[9][0].item() * hop_size\n",
    "wav_gen = wav_gen[0,0,:wav_gen_len]\n",
    "mel_gen = processor.get_mel_energy(wav_gen.detach().cpu().numpy())[0]\n",
    "\n",
    "# voice conversion\n",
    "input_mel = torch.from_numpy(mel_gen).unsqueeze(0)\n",
    "input_mel_lengths = torch.LongTensor([input_mel.size(-1)])\n",
    "input_mel_max_length = input_mel_lengths.max().item()\n",
    "output_gen_vc = model.voice_conversion(\n",
    "    mels=input_mel, \n",
    "    mel_lens=input_mel_lengths, \n",
    "    max_mel_len=input_mel_max_length, \n",
    "    sid_src=speaker_id, \n",
    "    sid_tgt=target_speaker_id)\n",
    "wav_gen_vc = output_gen_vc[0][0,0,:None]\n",
    "mel_gen_vc = processor.get_mel_energy(wav_gen_vc.detach().cpu().numpy())[0]\n",
    "\n",
    "print(\"Generated\")\n",
    "plt.imshow(mel_gen, origin='lower', aspect='auto')\n",
    "plt.show()\n",
    "display(ipd.Audio(wav_gen.detach().cpu().numpy(), rate=sr, normalize=False, autoplay=False))\n",
    "\n",
    "print(\"Converted\")\n",
    "plt.imshow(mel_gen_vc, origin='lower', aspect='auto')\n",
    "plt.show()\n",
    "display(ipd.Audio(wav_gen_vc.detach().cpu().numpy(), rate=sr, normalize=False, autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12f239-01b8-4d41-9e34-a3c36f420086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
